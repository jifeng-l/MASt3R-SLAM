import os
import torch
import inspect
import tensorrt as trt
import numpy as np
import pycuda.driver as cuda
from typing import Tuple, Dict, Any
from mast3r.model import AsymmetricMASt3R

# -----------------------------------------------------------------------------
# TensorRT logger
# -----------------------------------------------------------------------------
class TensorRTLogger(trt.ILogger):
    def __init__(self):
        super().__init__()
    def log(self, severity, msg):
        if severity == trt.Logger.ERROR:
            print(f"[ERROR] {msg}")
        elif severity == trt.Logger.WARNING:
            print(f"[WARNING] {msg}")
        elif severity == trt.Logger.INFO:
            print(f"[INFO] {msg}")
        elif severity == trt.Logger.VERBOSE:
            print(f"[VERBOSE] {msg}")

# -----------------------------------------------------------------------------
# ONNXâ€‘export wrapper
# -----------------------------------------------------------------------------
class ExportWrapper(torch.nn.Module):
    """
    Wraps AsymmetricMASt3R to simplify its output for ONNX export.
    It extracts and returns only the 'pts3d' tensor from the first result dictionary.
    """
    def __init__(self, model: AsymmetricMASt3R):
        super().__init__()
        self.model = model
        # Precompute and store output shapes for inference
        self.cached_output_shape = None
        self.initialized = False

    def _initialize_with_dummy_input(self, view1, view2):
        """Run model once to determine output shape and cache it"""
        print("[DEBUG] Initializing export wrapper with dummy input")
        in1 = {
            "img": view1,
            "instance": torch.zeros(view1.shape[0], dtype=torch.long, device=view1.device),
            "true_shape": torch.tensor(view1.shape[-2:], device=view1.device)[None].repeat(view1.shape[0], 1)
        }
        in2 = {
            "img": view2,
            "instance": torch.zeros(view2.shape[0], dtype=torch.long, device=view2.device),
            "true_shape": torch.tensor(view2.shape[-2:], device=view2.device)[None].repeat(view2.shape[0], 1)
        }
        with torch.no_grad():
            res1, _ = self.model(in1, in2) # We only care about res1 for 'pts3d'
            output_tensor = res1['pts3d']
        self.cached_output_shape = output_tensor.shape
        self.initialized = True
        print(f"[DEBUG] Cached output shape: {self.cached_output_shape}")

    def forward(self, view1: torch.Tensor, view2: torch.Tensor) -> torch.Tensor:
        if not self.initialized:
            self._initialize_with_dummy_input(view1, view2)

        # During actual export, return a tensor of zeros with the cached shape
        # This avoids re-running the complex model logic during tracing
        return torch.zeros(self.cached_output_shape, dtype=view1.dtype, device=view1.device)

# -----------------------------------------------------------------------------
# Converter
# -----------------------------------------------------------------------------
class MASt3RTensorRTConverter:
    def __init__(
        self,
        model_path: str,
        engine_path: str,
        precision: str = "fp16",
        max_workspace_size: int = 1 << 30,
        max_batch_size: int = 1,
        height: int = 512,
        width: int = 512,
        channels: int = 3,
    ):
        print(f"Initializing converter with model_path: {model_path}")
        self.model_path = model_path
        self.engine_path = engine_path
        self.precision = precision
        self.max_workspace_size = max_workspace_size
        self.max_batch_size = max_batch_size
        self.height = height
        self.width = width
        self.channels = channels

        # TensorRT setup
        self.logger = TensorRTLogger()
        self.builder = trt.Builder(self.logger)
        self.network = self.builder.create_network(
            1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)
        )
        self.config = self.builder.create_builder_config()

        # Precision flags
        if precision == "fp16" and self.builder.platform_has_fast_fp16:
            self.config.set_flag(trt.BuilderFlag.FP16)
        elif precision == "int8" and self.builder.platform_has_fast_int8:
            self.config.set_flag(trt.BuilderFlag.INT8)

        # Workspace size
        self.config.set_memory_pool_limit(
            trt.MemoryPoolType.WORKSPACE, self.max_workspace_size
        )

        # Optimization profile for dynamic batch
        profile = self.builder.create_optimization_profile()
        profile.set_shape(
            "view1",
            (1, self.channels, self.height, self.width),
            (self.max_batch_size, self.channels, self.height, self.width),
            (self.max_batch_size, self.channels, self.height, self.width),
        )
        profile.set_shape(
            "view2",
            (1, self.channels, self.height, self.width),
            (self.max_batch_size, self.channels, self.height, self.width),
            (self.max_batch_size, self.channels, self.height, self.width),
        )
        self.config.add_optimization_profile(profile)

    def convert_mast3r_model(self, input_shape: Tuple[int, int, int, int]):
        print(f"Starting model conversion with input shape: {input_shape}")
        onnx_path = os.path.splitext(self.model_path)[0] + ".onnx"
        wrapper, onnx_path = self._export_to_onnx(onnx_path, input_shape)

        # parse ONNX
        parser = trt.OnnxParser(self.network, self.logger)
        with open(onnx_path, "rb") as f:
            if not parser.parse(f.read()):
                for i in range(parser.num_errors):
                    print(parser.get_error(i))
                raise RuntimeError("ONNX parsing failed")

        # build engine
        engine = self.builder.build_engine(self.network, self.config)
        if engine is None:
            raise RuntimeError("Failed to build TensorRT engine")

        # serialize
        with open(self.engine_path, "wb") as f:
            f.write(engine.serialize())
        print(f"[INFO] TensorRT engine saved to {self.engine_path}")

    def _export_to_onnx(
        self, onnx_path: str, input_shape: Tuple[int, int, int, int]
    ) -> Tuple[ExportWrapper, str]:
        print(f"Loading model from {self.model_path}")
        ckpt = torch.load(self.model_path, map_location="cpu")
        if not isinstance(ckpt, dict) or not isinstance(ckpt.get("model"), dict):
            raise RuntimeError("Checkpoint must be a dict containing 'model'")

        state_dict = ckpt["model"]
        raw_args = ckpt.get("args", {})
        args_dict: Dict[str, Any] = {}
        if hasattr(raw_args, "__dict__"):
            args_dict = vars(raw_args).copy()
        elif isinstance(raw_args, dict):
            args_dict = raw_args.copy()

        # sanitize / defaults
        args_dict.pop("model", None)
        print(f"Raw args_dict before processing: {args_dict}")
        
        # Set required defaults
        args_dict.update({
            "img_size": (self.height, self.width),
            "patch_size": 16,
            "enc_depth": 24,
            "dec_depth": 12,
            "enc_embed_dim": 1024,
            "dec_embed_dim": 768,
            "enc_num_heads": 16,
            "dec_num_heads": 12,
            "pos_embed": "RoPE100",
            "head_type": "catmlp+dpt",
            "output_mode": "pts3d+desc24",
            "depth_mode": ("exp", float("-inf"), float("inf")),
            "conf_mode": ("exp", 1, float("inf")),
            "patch_embed_cls": "PatchEmbedDust3R",
            "two_confs": True,
            "desc_conf_mode": ("exp", 0, float("inf")),
            "landscape_only": False
        })
        
        print(f"Final args_dict with defaults: {args_dict}")

        # Manually ensure all required parameters are included
        required_params = {
            "img_size": args_dict["img_size"],
            "patch_size": args_dict["patch_size"],
            "enc_depth": args_dict["enc_depth"],
            "dec_depth": args_dict["dec_depth"],
            "enc_embed_dim": args_dict["enc_embed_dim"],
            "dec_embed_dim": args_dict["dec_embed_dim"],
            "enc_num_heads": args_dict["enc_num_heads"],
            "dec_num_heads": args_dict["dec_num_heads"],
            "pos_embed": args_dict["pos_embed"],
            "head_type": args_dict["head_type"],
            "output_mode": args_dict["output_mode"],
            "depth_mode": args_dict["depth_mode"],
            "conf_mode": args_dict["conf_mode"],
            "patch_embed_cls": args_dict["patch_embed_cls"],
            "two_confs": args_dict["two_confs"],
            "desc_conf_mode": args_dict["desc_conf_mode"],
            "landscape_only": args_dict["landscape_only"]
        }

        print(f"Required parameters for model initialization: {required_params}")
        model = AsymmetricMASt3R(**required_params)
        model.load_state_dict(state_dict)
        model.eval()

        # wrap for ONNX export
        wrapper = ExportWrapper(model).eval()

        # create dummy inputs
        print("Creating dummy inputs for stereo pair and exporting to ONNX...")
        dummy1 = torch.randn(input_shape)
        dummy2 = torch.randn(input_shape)

        torch.onnx.export(
            wrapper,
            (dummy1, dummy2),
            onnx_path,
            export_params=True,
            opset_version=13,
            do_constant_folding=True,
            input_names=["view1", "view2"],
            output_names=["output"],
            dynamic_axes={
                "view1": {0: "batch_size"},
                "view2": {0: "batch_size"},
                "output": {0: "batch_size"},
            },
        )
        print(f"[INFO] ONNX model exported to {onnx_path}")
        return wrapper, onnx_path

# -----------------------------------------------------------------------------
# (Optional) Inference helper
# -----------------------------------------------------------------------------
class MASt3RTensorRTInference:
    def __init__(self, engine_path: str):
        cuda.init()
        self.logger = TensorRTLogger()
        self.runtime = trt.Runtime(self.logger)
        with open(engine_path, "rb") as f:
            self.engine = self.runtime.deserialize_cuda_engine(f.read())
        self.context = self.engine.create_execution_context()

    def infer(self, input_tensor: np.ndarray) -> np.ndarray:
        d_input = cuda.mem_alloc(input_tensor.nbytes)
        output_shape = tuple(self.engine.get_binding_shape(1))
        d_output = cuda.mem_alloc(np.prod(output_shape) * np.dtype(np.float32).itemsize)

        cuda.memcpy_htod(d_input, input_tensor)
        self.context.execute_v2(bindings=[int(d_input), int(d_output)])
        out = np.empty(output_shape, dtype=np.float32)
        cuda.memcpy_dtoh(out, d_output)
        return out
